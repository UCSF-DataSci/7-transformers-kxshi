{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b192f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kevxs\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kevxs\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevxs\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kevxs\\anaconda3\\lib\\site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevxs\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# Additional packages for LLM API interaction\n",
    "%pip install requests\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not best to hardcode this, but for now it works and I don't know how to get it from the environment\n",
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a01d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the Hugging Face API\n",
    "    \n",
    "    Args:\n",
    "        payload: Dictionary containing the query parameters\n",
    "        \n",
    "    Returns:\n",
    "        The API response\n",
    "    \"\"\"\n",
    "    # Use requests.post to send the query to the API_URL\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    print(\"Raw text response:\", response.text)  # Debug line\n",
    "    # Return the response\n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError:\n",
    "        print(\"Failed to parse JSON.\")\n",
    "        return {\"error\": \"Non-JSON response\"}\n",
    "\n",
    "    \n",
    "# Test the query function\n",
    "test_payload = {\"inputs\": \"What are the symptoms of diabetes?\"}\n",
    "response = query(test_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "def get_response(prompt, model_name=\"HuggingFaceH4/zephyr-7b-beta\", api_key=API_TOKEN):\n",
    "    \"\"\"\n",
    "    Get a response from the model\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication (optional for some models)\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # Set up the API URL and headers\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    # Create a payload with the prompt\n",
    "    payload = {\"inputs\": prompt}\n",
    "    # Send the payload to the API\n",
    "    # Extract and return the generated text from the response\n",
    "    # Handle any errors that might occur\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        output = response.json()\n",
    "        if isinstance(output, list) and 'generated_text' in output[0]:\n",
    "            return output[0]['generated_text'].strip()\n",
    "        elif isinstance(output, dict) and 'generated_text' in output:\n",
    "            return output['generated_text'].strip()\n",
    "        return str(output)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"[ERROR] API request failed: {e}\"\n",
    "\n",
    "def run_chat(model_name, api_key):\n",
    "    \"\"\"Run an interactive chat session\"\"\"\n",
    "    print(\"Welcome to the Simple LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        response = get_response(user_input, model_name=model_name, api_key=api_key)\n",
    "        print(f\"Response: {response}\")\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM\")\n",
    "    \n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"HuggingFaceH4/zephyr-7b-beta\", help=\"Name of the model to use\")\n",
    "    parser.add_argument(\"--api_key\", type=str, default=API_TOKEN, help=\"API key for authentication\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    run_chat(model_name=args.model_name, api_key=args.api_key)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "def get_response(prompt, history=None, model_name=\"HuggingFaceH4/zephyr-7b-beta\", api_key=API_TOKEN, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # Just chuck everything together in the prompt\n",
    "    context_prompt = \"\"\n",
    "    for past_prompt, past_response in history[-history_length:]:\n",
    "        context_prompt += f\"User: {past_prompt}\\nResponse: {past_response}\\n\"\n",
    "    context_prompt += f\"User: {prompt}\\nResponse:\"\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\"inputs\": context_prompt}\n",
    "\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        output = response.json()\n",
    "        if isinstance(output, list) and 'generated_text' in output[0]:\n",
    "            return output[0]['generated_text'].strip()\n",
    "        elif isinstance(output, dict) and 'generated_text' in output:\n",
    "            return output['generated_text'].strip()\n",
    "        return str(output)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"[ERROR] API request failed: {e}\"\n",
    "\n",
    "def run_chat(model_name, api_key, history_length):\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # Get response using conversation history\n",
    "        response = get_response(\n",
    "            prompt=user_input,\n",
    "            history=history,\n",
    "            model_name=model_name,\n",
    "            api_key=api_key,\n",
    "            history_length=history_length\n",
    "        )\n",
    "        # Update history\n",
    "        history.append((user_input, response))\n",
    "        # Print the response\n",
    "        print(f\"LLM: {response}\\n\")\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"HuggingFaceH4/zephyr-7b-beta\", help=\"Hugging Face model name\")\n",
    "    parser.add_argument(\"--api_key\", type=str, default=\"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\", help=\"Hugging Face API token\")\n",
    "    parser.add_argument(\"--history_length\", type=int, default=3, help=\"Number of previous questions and answers to include in context\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    run_chat(model_name=args.model_name, api_key=args.api_key, history_length=args.history_length)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
